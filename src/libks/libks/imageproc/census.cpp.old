#include "libks/imageproc/census-inl.h"

namespace ks {
	using namespace std;
	using namespace cv;

	template <>
	void Census::transform9x3<char>(const Mat_<char>& input, Mat_<unsigned int>* output) {
		transform9x3SSE(input, output);
	}
	
	// Simpler but slightly slower implementation
	/*void Census::transform9x3SSE(const Mat_<char>& input, Mat_<unsigned int>* output) {
		for(int y=1; y<input.rows-1; y++) {
			for(int x=1; x<input.cols-1; x++) {
				// Create a vector for 13 comparisons to the center value
				char c = input(y, x);
				v16qi centoid = {c, c, c, c, c, c, c, c, c, c, c, c, c, 0, 0, 0}; 
				
				// Compare first set of pixels
				const char* row1Ptr = &input(y-1, x-4);
				const char* row2Ptr = row1Ptr + input.step;
				v16qi compSet1 = {*(row2Ptr+3), *(row2Ptr+2), *(row2Ptr+1), *row2Ptr,
					*(row1Ptr+8), *(row1Ptr+7), *(row1Ptr+6), *(row1Ptr+5), *(row1Ptr+4),
					*(row1Ptr+3), *(row1Ptr+2), *(row1Ptr+1), *row1Ptr, 1, 1, 1};
					
				v16qi compRes1 = __builtin_ia32_pcmpgtb128(centoid, compSet1);
				int census1 = __builtin_ia32_pmovmskb128(compRes1);
				
				// Compare second set of pixels
				row1Ptr = &input(y, x-4);
				row2Ptr = row1Ptr + input.step;
				v16qi compSet2 = {*(row2Ptr+8), *(row2Ptr+7), *(row2Ptr+6), *(row2Ptr+5),
					*(row2Ptr+4), *(row2Ptr+3), *(row2Ptr+2), *(row2Ptr+1), *row2Ptr,
					*(row1Ptr+8), *(row1Ptr+7), *(row1Ptr+6), *(row1Ptr+5), 1, 1, 1};
					
				v16qi compRes2 = __builtin_ia32_pcmpgtb128(centoid, compSet2);
				int census2 = __builtin_ia32_pmovmskb128(compRes2);

				(*output)(y,x) = (census1 << 14) | census2;
			}
		}
	}*/
	
	void Census::transform9x3SSE(const Mat_<char>& input, Mat_<unsigned int>* output) {
		v16qi censusBits[3][9];
		
		// Predeclare required constants
		const v16qi const01 = SIMD::scalar16(0x01), const02 = SIMD::scalar16(0x02), const04 = SIMD::scalar16(0x04),
			const08 = SIMD::scalar16(0x08), const10 = SIMD::scalar16(0x10), const20 = SIMD::scalar16(0x20),
			const40 = SIMD::scalar16(0x40), const80 = SIMD::scalar16(0x80);
		
		// We skip one row at the beginning and end to avoid range checking
		//#pragma omp parallel for default(shared) private(censusBits)
		for(int y=2; y<input.rows-2; y++) {
		
			for(int x=0; x<input.cols; x+=16) {
				// Get 16 centoids
				v16qi centoid = __builtin_ia32_loaddqu((char*)&(input)(y,x));
				
				// Comparisons results are aligned in 8-bit groups and combined at the bottom
				
				// Row 1
				const char* rowPtr = &input(y-1, x-4);
				// Byte 4 starts
				censusBits[0][0] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr)) & const04;
				censusBits[0][1] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 1)) & const02;
				censusBits[0][2] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 2)) & const01;
				// Byte 3 starts
				censusBits[0][3] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 3)) & const80;
				censusBits[0][4] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 4)) & const40;
				censusBits[0][5] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 5)) & const20;
				censusBits[0][6] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 6)) & const10;
				censusBits[0][7] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 7)) & const08;
				censusBits[0][8] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 8)) & const04;
				
				// Row 2
				rowPtr += input.step;
				censusBits[1][0] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr)) & const02;
				censusBits[1][1] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 1)) & const01;
				// Byte 2 starts
				censusBits[1][2] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 2)) & const80;
				censusBits[1][3] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 3)) & const40;
#ifdef KS_CENSUS_COMPARE_CENTER
				censusBits[1][4] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 4)) & const20;
#endif
				censusBits[1][5] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 5)) & const10;
				censusBits[1][6] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 6)) & const08;
				censusBits[1][7] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 7)) & const04;
				censusBits[1][8] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 8)) & const02;
				
				// Row 3
				rowPtr += input.step;
				censusBits[2][0] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr)) & const01;
				// Byte 1 starts
				censusBits[2][1] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 1)) & const80;
				censusBits[2][2] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 2)) & const40;
				censusBits[2][3] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 3)) & const20;
				censusBits[2][4] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 4)) & const10;
				censusBits[2][5] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 5)) & const08;
				censusBits[2][6] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 6)) & const04;
				censusBits[2][7] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 7)) & const02;
				censusBits[2][8] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 8)) & const01;
				
				// Combine bits to bytes
				v16qi byte4 = censusBits[0][0] | censusBits[0][1] | censusBits[0][2];
				v16qi byte3 = censusBits[0][3] | censusBits[0][4] | censusBits[0][5] | censusBits[0][6] | censusBits[0][7] | censusBits[0][8]
					| censusBits[1][0] | censusBits[1][1];
				v16qi byte2 = censusBits[1][2] | censusBits[1][3]
#ifdef KS_CENSUS_COMPARE_CENTER
					| censusBits[1][4]
#endif
					| censusBits[1][5] | censusBits[1][6] | censusBits[1][7] | censusBits[1][8] | censusBits[2][0];
				v16qi byte1 = censusBits[2][1] | censusBits[2][2] | censusBits[2][3] | censusBits[2][4] | censusBits[2][5] | censusBits[2][6]
					| censusBits[2][7] | censusBits[2][8];
					
				storeSSEVec(byte4, byte3, byte2, byte1, (char*) &(*output)(y,x));
			}
		}
	}
	
	template <>
	void Census::transform5x5<char>(const Mat_<char>& input, Mat_<unsigned int>* output) {
		transform5x5SSE(input, output);
	}
	
	void Census::transform5x5SSE(const Mat_<char>& input, Mat_<unsigned int>* output) {
		v16qi censusBits[5][5];
		
		// Predeclare required constants
		const v16qi const01 = SIMD::scalar16(0x01), const02 = SIMD::scalar16(0x02), const04 = SIMD::scalar16(0x04),
			const08 = SIMD::scalar16(0x08), const10 = SIMD::scalar16(0x10), const20 = SIMD::scalar16(0x20),
			const40 = SIMD::scalar16(0x40), const80 = SIMD::scalar16(0x80);

		// We skip one row at the beginning and end to avoid range checking
		//#pragma omp parallel for default(shared) shared(output) private(censusBits)
		for(int y=2; y<input.rows-2; y++)
			for(int x=0; x<input.cols; x+=16) {
				// Get 16 centoids
				v16qi centoid = __builtin_ia32_loaddqu((char*)&(input)(y,x));
				
				// Comparisons results are aligned in 8-bit groups and combined at the bottom
				
				// Row 1
				const char* rowPtr = &input(y-2, x-2);
				// Byte 4 starts
				censusBits[0][0] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr)) & const01;
				// Byte 3 starts
				censusBits[0][1] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 1)) & const80;
				censusBits[0][2] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 2)) & const40;
				censusBits[0][3] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 3)) & const20;
				censusBits[0][4] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 4)) & const10;
				
				// Row 2
				rowPtr += input.step;
				censusBits[1][0] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr)) & const08;
				censusBits[1][1] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 1)) & const04;
				censusBits[1][2] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 2)) & const02;
				censusBits[1][3] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 3)) & const01;
				// Byte 2 starts			
				censusBits[1][4] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 4)) & const80;
				
				// Row 3
				rowPtr += input.step;
				censusBits[2][0] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr)) & const40;
				censusBits[2][1] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 1)) & const20;
#ifdef KS_CENSUS_COMPARE_CENTER
				censusBits[2][2] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 2)) & const10;
#endif
				censusBits[2][3] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 3)) & const08;
				censusBits[2][4] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 4)) & const04;
				
				// Row 4
				rowPtr += input.step;
				censusBits[3][0] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr)) & const02;
				censusBits[3][1] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 1)) & const01;
				// Byte 1 starts
				censusBits[3][2] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 2)) & const80;
				censusBits[3][3] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 3)) & const40;
				censusBits[3][4] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 4)) & const20;
				
				// Row 5
				rowPtr += input.step;
				censusBits[4][0] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr)) & const10;
				censusBits[4][1] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 1)) & const08;
				censusBits[4][2] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 2)) & const04;
				censusBits[4][3] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 3)) & const02;
				censusBits[4][4] = __builtin_ia32_pcmpgtb128(centoid, __builtin_ia32_loaddqu(rowPtr + 4)) & const01;
				
				// Combine bits to bytes
				v16qi byte4 = censusBits[0][0];
				v16qi byte3 = censusBits[0][1] | censusBits[0][2] | censusBits[0][3] | censusBits[0][4] | censusBits[1][0] | censusBits[1][1]
					| censusBits[1][2] | censusBits[1][3];
				v16qi byte2 = censusBits[1][4] | censusBits[2][0] | censusBits[2][1]
#ifdef KS_CENSUS_COMPARE_CENTER
					| censusBits[2][2]
#endif
					| censusBits[2][3] | censusBits[2][4] | censusBits[3][0] | censusBits[3][1];
				v16qi byte1 = censusBits[3][2] | censusBits[3][3] | censusBits[3][4] | censusBits[4][0] | censusBits[4][1] | censusBits[4][2]
					| censusBits[4][3] | censusBits[4][4];
					
				storeSSEVec(byte4, byte3, byte2, byte1, (char*) &(*output)(y,x));
			}
	}
	
	__always_inline void Census::storeSSEVec(const v16qi& byte4, const v16qi& byte3, const v16qi& byte2, const v16qi& byte1, char* dst) {
		// Combine bytes to shorts
		v8hi high1 = (v8hi) __builtin_ia32_punpcklbw128(byte3, byte4);
		v8hi high2 = (v8hi) __builtin_ia32_punpckhbw128(byte3, byte4);
		v8hi low1 = (v8hi) __builtin_ia32_punpcklbw128(byte1, byte2);
		v8hi low2 = (v8hi) __builtin_ia32_punpckhbw128(byte1, byte2);
		
		// Combine shorts to ints
		__builtin_ia32_storedqu(dst, (v16qi)__builtin_ia32_punpcklwd128(low1, high1));
		__builtin_ia32_storedqu(dst + 4*4, (v16qi)__builtin_ia32_punpckhwd128(low1, high1));
		__builtin_ia32_storedqu(dst + 8*4, (v16qi)__builtin_ia32_punpcklwd128(low2, high2));
		__builtin_ia32_storedqu(dst + 12*4, (v16qi)__builtin_ia32_punpckhwd128(low2, high2));
	}
}
